{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Helmet Detection\n## By Saleh Shmali   \nEmail: salehshmali@outlook.com  \nPhone: (+971) 503756709\n\nDescription: \n## Dfine the problem\nfor helmet detection  i downloaded videos for Myanmar street where is a lot of motocycle in it's streets then i annotated labels just like this:\n\nDHelmet, DHelmetP1Helmet, DHelmetP1NoHelmet, DN0HelmetP1NoHelmetP2NoHelmet, DNoHelmet, DNoHelmetP0NoHelmet, DNoHelmetP1Helmet, DNoHelmetP1NoHelmet, DNoHelmetP1NoHelmetP2NoHelmet, DNoHelmetPoNoHelmetP1NoHelmet\n\nWhich D:Driver , P1:First Rider , P2:Second Rider , P0 :Kid Rider in front of the driver\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:50:12.413532Z","iopub.execute_input":"2022-03-31T08:50:12.413797Z","iopub.status.idle":"2022-03-31T08:50:12.417789Z","shell.execute_reply.started":"2022-03-31T08:50:12.413768Z","shell.execute_reply":"2022-03-31T08:50:12.417002Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Download Helmet Detection Dataset\nI used RoboFlow online tool annotate dataset and Export train and validation data to TFRecord files, this powerful tool offers url link to download dataset on Colab directly. ","metadata":{}},{"cell_type":"code","source":"! curl -L \"https://app.roboflow.com/ds/SuzuRDNRwo?key=YgUpOynK1I\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:26:57.940425Z","iopub.execute_input":"2022-03-31T08:26:57.940680Z","iopub.status.idle":"2022-03-31T08:27:01.954314Z","shell.execute_reply.started":"2022-03-31T08:26:57.940650Z","shell.execute_reply":"2022-03-31T08:27:01.953533Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing\n\nImpliminting some functions for image and bbox prepearring","metadata":{}},{"cell_type":"code","source":"\ndef swap_xy(boxes):\n    \"\"\"Swaps order the of x and y coordinates of the boxes.\n\n    Arguments:\n      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n\n    Returns:\n      swapped boxes with shape same as that of boxes.\n    \"\"\"\n    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n\n\ndef convert_to_xywh(boxes):\n    \"\"\"Changes the box format to center, width and height.\n\n    Arguments:\n      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n        representing bounding boxes where each box is of the format\n        `[xmin, ymin, xmax, ymax]`.\n\n    Returns:\n      converted boxes with shape same as that of boxes.\n    \"\"\"\n    return tf.concat(\n        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n        axis=-1,\n    )\n\n\ndef convert_to_corners(boxes):\n    \"\"\"Changes the box format to corner coordinates\n\n    Arguments:\n      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n        representing bounding boxes where each box is of the format\n        `[x, y, width, height]`.\n\n    Returns:\n      converted boxes with shape same as that of boxes.\n    \"\"\"\n    return tf.concat(\n        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n        axis=-1,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:12.084454Z","iopub.execute_input":"2022-03-31T08:27:12.084726Z","iopub.status.idle":"2022-03-31T08:27:12.093472Z","shell.execute_reply.started":"2022-03-31T08:27:12.084696Z","shell.execute_reply":"2022-03-31T08:27:12.092819Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def compute_iou(boxes1, boxes2):\n    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n\n    Arguments:\n      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n        where each box is of the format `[x, y, width, height]`.\n        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n        where each box is of the format `[x, y, width, height]`.\n\n    Returns:\n      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n        jth column holds the IOU between ith box and jth box from\n        boxes1 and boxes2 respectively.\n    \"\"\"\n    boxes1_corners = convert_to_corners(boxes1)\n    boxes2_corners = convert_to_corners(boxes2)\n    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n    intersection = tf.maximum(0.0, rd - lu)\n    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n    union_area = tf.maximum(\n        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n    )\n    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n\n\ndef visualize_detections(\n    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n):\n    \"\"\"Visualize Detections\"\"\"\n    image = np.array(image, dtype=np.uint8)\n    plt.figure(figsize=figsize)\n    plt.axis(\"off\")\n    plt.imshow(image)\n    ax = plt.gca()\n    for box, _cls, score in zip(boxes, classes, scores):\n        text = \"{}: {:.2f}\".format(_cls, score)\n        x1, y1, x2, y2 = box\n        w, h = x2 - x1, y2 - y1\n        patch = plt.Rectangle(\n            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n        )\n        ax.add_patch(patch)\n        ax.text(\n            x1,\n            y1,\n            text,\n            bbox={\"facecolor\": color, \"alpha\": 0.4},\n            clip_box=ax.clipbox,\n            clip_on=True,\n        )\n    plt.show()\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:14.544438Z","iopub.execute_input":"2022-03-31T08:27:14.545288Z","iopub.status.idle":"2022-03-31T08:27:14.568415Z","shell.execute_reply.started":"2022-03-31T08:27:14.545220Z","shell.execute_reply":"2022-03-31T08:27:14.567060Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def random_flip_horizontal(image, boxes):\n    \"\"\"Flips image and boxes horizontally with 50% chance\n\n    Arguments:\n      image: A 3-D tensor of shape `(height, width, channels)` representing an\n        image.\n      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n        having normalized coordinates.\n\n    Returns:\n      Randomly flipped image and boxes\n    \"\"\"\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_left_right(image)\n        boxes = tf.stack(\n            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n        )\n    return image, boxes\n\n\ndef resize_and_pad_image(\n    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n):\n    \"\"\"Resizes and pads image while preserving aspect ratio.\n\n    1. Resizes images so that the shorter side is equal to `min_side`\n    2. If the longer side is greater than `max_side`, then resize the image\n      with longer side equal to `max_side`\n    3. Pad with zeros on right and bottom to make the image shape divisible by\n    `stride`\n\n    Arguments:\n      image: A 3-D tensor of shape `(height, width, channels)` representing an\n        image.\n      min_side: The shorter side of the image is resized to this value, if\n        `jitter` is set to None.\n      max_side: If the longer side of the image exceeds this value after\n        resizing, the image is resized such that the longer side now equals to\n        this value.\n      jitter: A list of floats containing minimum and maximum size for scale\n        jittering. If available, the shorter side of the image will be\n        resized to a random value in this range.\n      stride: The stride of the smallest feature map in the feature pyramid.\n        Can be calculated using `image_size / feature_map_size`.\n\n    Returns:\n      image: Resized and padded image.\n      image_shape: Shape of the image before padding.\n      ratio: The scaling factor used to resize the image\n    \"\"\"\n    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n    if jitter is not None:\n        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n    ratio = min_side / tf.reduce_min(image_shape)\n    if ratio * tf.reduce_max(image_shape) > max_side:\n        ratio = max_side / tf.reduce_max(image_shape)\n    image_shape = ratio * image_shape\n    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n    padded_image_shape = tf.cast(\n        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n    )\n    image = tf.image.pad_to_bounding_box(\n        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n    )\n    return image, image_shape, ratio\n\n\ndef preprocess_data(sample):\n    image=sample['image']\n    bbox=sample['bbox']\n    class_id=sample['label']\n    bbox = swap_xy(bbox)\n    image, bbox = random_flip_horizontal(image, bbox)\n    image, image_shape, _ = resize_and_pad_image(image)\n\n    bbox = tf.stack(\n        [\n            bbox[:, 0] * image_shape[1],\n            bbox[:, 1] * image_shape[0],\n            bbox[:, 2] * image_shape[1],\n            bbox[:, 3] * image_shape[0],\n        ],\n        axis=-1,\n    )\n    bbox = convert_to_xywh(bbox)\n    return image, bbox, class_id\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:16.960270Z","iopub.execute_input":"2022-03-31T08:27:16.960531Z","iopub.status.idle":"2022-03-31T08:27:16.974451Z","shell.execute_reply.started":"2022-03-31T08:27:16.960500Z","shell.execute_reply":"2022-03-31T08:27:16.973778Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DecodePredictions(tf.keras.layers.Layer):\n    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n\n    Attributes:\n      num_classes: Number of classes in the dataset\n      confidence_threshold: Minimum class probability, below which detections\n        are pruned.\n      nms_iou_threshold: IOU threshold for the NMS operation\n      max_detections_per_class: Maximum number of detections to retain per\n       class.\n      max_detections: Maximum number of detections to retain across all\n        classes.\n      box_variance: The scaling factors used to scale the bounding box\n        predictions.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes=80,\n        confidence_threshold=0.05,\n        nms_iou_threshold=0.5,\n        max_detections_per_class=100,\n        max_detections=100,\n        box_variance=[0.1, 0.1, 0.2, 0.2],\n        **kwargs\n    ):\n        super(DecodePredictions, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.confidence_threshold = confidence_threshold\n        self.nms_iou_threshold = nms_iou_threshold\n        self.max_detections_per_class = max_detections_per_class\n        self.max_detections = max_detections\n\n        self._anchor_box = AnchorBox()\n        self._box_variance = tf.convert_to_tensor(\n            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n        )\n\n    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n        boxes = box_predictions * self._box_variance\n        boxes = tf.concat(\n            [\n                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n            ],\n            axis=-1,\n        )\n        boxes_transformed = convert_to_corners(boxes)\n        return boxes_transformed\n\n    def call(self, images, predictions):\n        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n        box_predictions = predictions[:, :, :4]\n        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n\n        return tf.image.combined_non_max_suppression(\n            tf.expand_dims(boxes, axis=2),\n            cls_predictions,\n            self.max_detections_per_class,\n            self.max_detections,\n            self.nms_iou_threshold,\n            self.confidence_threshold,\n            clip_boxes=False,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:30.213186Z","iopub.execute_input":"2022-03-31T08:27:30.213467Z","iopub.status.idle":"2022-03-31T08:27:31.225976Z","shell.execute_reply.started":"2022-03-31T08:27:30.213437Z","shell.execute_reply":"2022-03-31T08:27:31.225241Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class AnchorBox:\n    \"\"\"Generates anchor boxes.\n\n    This class has operations to generate anchor boxes for feature maps at\n    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n    format `[x, y, width, height]`.\n\n    Attributes:\n      aspect_ratios: A list of float values representing the aspect ratios of\n        the anchor boxes at each location on the feature map\n      scales: A list of float values representing the scale of the anchor boxes\n        at each location on the feature map.\n      num_anchors: The number of anchor boxes at each location on feature map\n      areas: A list of float values representing the areas of the anchor\n        boxes for each feature map in the feature pyramid.\n      strides: A list of float value representing the strides for each feature\n        map in the feature pyramid.\n    \"\"\"\n\n    def __init__(self):\n        self.aspect_ratios = [0.5, 1.0, 2.0]\n        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n\n        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n        self._strides = [2 ** i for i in range(3, 8)]\n        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n        self._anchor_dims = self._compute_dims()\n\n    def _compute_dims(self):\n        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n        of the feature pyramid.\n        \"\"\"\n        anchor_dims_all = []\n        for area in self._areas:\n            anchor_dims = []\n            for ratio in self.aspect_ratios:\n                anchor_height = tf.math.sqrt(area / ratio)\n                anchor_width = area / anchor_height\n                dims = tf.reshape(\n                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n                )\n                for scale in self.scales:\n                    anchor_dims.append(scale * dims)\n            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n        return anchor_dims_all\n\n    def _get_anchors(self, feature_height, feature_width, level):\n        \"\"\"Generates anchor boxes for a given feature map size and level\n\n        Arguments:\n          feature_height: An integer representing the height of the feature map.\n          feature_width: An integer representing the width of the feature map.\n          level: An integer representing the level of the feature map in the\n            feature pyramid.\n\n        Returns:\n          anchor boxes with the shape\n          `(feature_height * feature_width * num_anchors, 4)`\n        \"\"\"\n        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n        centers = tf.expand_dims(centers, axis=-2)\n        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n        dims = tf.tile(\n            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n        )\n        anchors = tf.concat([centers, dims], axis=-1)\n        return tf.reshape(\n            anchors, [feature_height * feature_width * self._num_anchors, 4]\n        )\n\n    def get_anchors(self, image_height, image_width):\n        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n\n        Arguments:\n          image_height: Height of the input image.\n          image_width: Width of the input image.\n\n        Returns:\n          anchor boxes for all the feature maps, stacked as a single tensor\n            with shape `(total_anchors, 4)`\n        \"\"\"\n        anchors = [\n            self._get_anchors(\n                tf.math.ceil(image_height / 2 ** i),\n                tf.math.ceil(image_width / 2 ** i),\n                i,\n            )\n            for i in range(3, 8)\n        ]\n        return tf.concat(anchors, axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:31.519008Z","iopub.execute_input":"2022-03-31T08:27:31.519331Z","iopub.status.idle":"2022-03-31T08:27:31.535866Z","shell.execute_reply.started":"2022-03-31T08:27:31.519302Z","shell.execute_reply":"2022-03-31T08:27:31.535276Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LabelEncoder:\n    \"\"\"Transforms the raw labels into targets for training.\n\n    This class has operations to generate targets for a batch of samples which\n    is made up of the input images, bounding boxes for the objects present and\n    their class ids.\n\n    Attributes:\n      anchor_box: Anchor box generator to encode the bounding boxes.\n      box_variance: The scaling factors used to scale the bounding box targets.\n    \"\"\"\n\n    def __init__(self):\n        self._anchor_box = AnchorBox()\n        self._box_variance = tf.convert_to_tensor(\n            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n        )\n\n    def _match_anchor_boxes(\n        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n    ):\n        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n\n        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n          to get a `(M, N)` shaped matrix.\n        2. The ground truth box with the maximum IOU in each row is assigned to\n          the anchor box provided the IOU is greater than `match_iou`.\n        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n          box is assigned with the background class.\n        4. The remaining anchor boxes that do not have any class assigned are\n          ignored during training.\n\n        Arguments:\n          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n            representing all the anchor boxes for a given input image shape,\n            where each anchor box is of the format `[x, y, width, height]`.\n          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n            the ground truth boxes, where each box is of the format\n            `[x, y, width, height]`.\n          match_iou: A float value representing the minimum IOU threshold for\n            determining if a ground truth box can be assigned to an anchor box.\n          ignore_iou: A float value representing the IOU threshold under which\n            an anchor box is assigned to the background class.\n\n        Returns:\n          matched_gt_idx: Index of the matched object\n          positive_mask: A mask for anchor boxes that have been assigned ground\n            truth boxes.\n          ignore_mask: A mask for anchor boxes that need to by ignored during\n            training\n        \"\"\"\n        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n        max_iou = tf.reduce_max(iou_matrix, axis=1)\n        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n        positive_mask = tf.greater_equal(max_iou, match_iou)\n        negative_mask = tf.less(max_iou, ignore_iou)\n        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n        return (\n            matched_gt_idx,\n            tf.cast(positive_mask, dtype=tf.float32),\n            tf.cast(ignore_mask, dtype=tf.float32),\n        )\n\n    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n        box_target = tf.concat(\n            [\n                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n            ],\n            axis=-1,\n        )\n        box_target = box_target / self._box_variance\n        return box_target\n\n    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n        \"\"\"Creates box and classification targets for a single sample\"\"\"\n        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n            anchor_boxes, gt_boxes\n        )\n        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n        cls_target = tf.where(\n            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n        )\n        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n        cls_target = tf.expand_dims(cls_target, axis=-1)\n        label = tf.concat([box_target, cls_target], axis=-1)\n        return label\n\n    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n        \"\"\"Creates box and classification targets for a batch\"\"\"\n        images_shape = tf.shape(batch_images)\n        batch_size = images_shape[0]\n\n        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n        for i in range(batch_size):\n            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n            labels = labels.write(i, label)\n        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n        return batch_images, labels.stack()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:32.812882Z","iopub.execute_input":"2022-03-31T08:27:32.813310Z","iopub.status.idle":"2022-03-31T08:27:32.831366Z","shell.execute_reply.started":"2022-03-31T08:27:32.813269Z","shell.execute_reply":"2022-03-31T08:27:32.830473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_backbone():\n    #Builds ResNet50 with pre-trained imagenet weights\n    backbone = keras.applications.ResNet50(\n        include_top=False, input_shape=[None, None, 3]\n    )\n    c3_output, c4_output, c5_output = [\n        backbone.get_layer(layer_name).output\n        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n    ]\n    return keras.Model(\n        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n    )","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:34.147880Z","iopub.execute_input":"2022-03-31T08:27:34.148294Z","iopub.status.idle":"2022-03-31T08:27:34.154447Z","shell.execute_reply.started":"2022-03-31T08:27:34.148252Z","shell.execute_reply":"2022-03-31T08:27:34.153759Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nclass FeaturePyramid(keras.layers.Layer):\n    #Builds the Feature Pyramid with the feature maps from the backbone.\n\n    def __init__(self, backbone=None, **kwargs):\n        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n        self.backbone = backbone if backbone else get_backbone()\n        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.upsample_2x = keras.layers.UpSampling2D(2)\n\n    def call(self, images, training=False):\n        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n        p3_output = self.conv_c3_1x1(c3_output)\n        p4_output = self.conv_c4_1x1(c4_output)\n        p5_output = self.conv_c5_1x1(c5_output)\n        p4_output = p4_output + self.upsample_2x(p5_output)\n        p3_output = p3_output + self.upsample_2x(p4_output)\n        p3_output = self.conv_c3_3x3(p3_output)\n        p4_output = self.conv_c4_3x3(p4_output)\n        p5_output = self.conv_c5_3x3(p5_output)\n        p6_output = self.conv_c6_3x3(c5_output)\n        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n        return p3_output, p4_output, p5_output, p6_output, p7_output\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:35.850572Z","iopub.execute_input":"2022-03-31T08:27:35.851002Z","iopub.status.idle":"2022-03-31T08:27:35.862555Z","shell.execute_reply.started":"2022-03-31T08:27:35.850961Z","shell.execute_reply":"2022-03-31T08:27:35.861877Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ndef build_head(output_filters, bias_init):\n    # Builds the class/box predictions head.\n\n    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n    for _ in range(4):\n        head.add(\n            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n        )\n        head.add(keras.layers.ReLU())\n    head.add(\n        keras.layers.Conv2D(\n            output_filters,\n            3,\n            1,\n            padding=\"same\",\n            kernel_initializer=kernel_init,\n            bias_initializer=bias_init,\n        )\n    )\n    return head","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:37.565217Z","iopub.execute_input":"2022-03-31T08:27:37.565961Z","iopub.status.idle":"2022-03-31T08:27:37.572456Z","shell.execute_reply.started":"2022-03-31T08:27:37.565926Z","shell.execute_reply":"2022-03-31T08:27:37.571765Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Network(keras.Model):\n    # A subclassed Keras model implementing the RetinaNet architecture.\n\n    def __init__(self, num_classes, backbone=None, **kwargs):\n        super(Network, self).__init__(name=\"Network\", **kwargs)\n        self.fpn = FeaturePyramid(backbone)\n        self.num_classes = num_classes\n\n        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n        self.cls_head = build_head(9 * num_classes, prior_probability)\n        self.box_head = build_head(9 * 4, \"zeros\")\n\n    def call(self, image, training=False):\n        features = self.fpn(image, training=training)\n        N = tf.shape(image)[0]\n        cls_outputs = []\n        box_outputs = []\n        for feature in features:\n            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n            cls_outputs.append(\n                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n            )\n        cls_outputs = tf.concat(cls_outputs, axis=1)\n        box_outputs = tf.concat(box_outputs, axis=1)\n        return tf.concat([box_outputs, cls_outputs], axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:39.103144Z","iopub.execute_input":"2022-03-31T08:27:39.103672Z","iopub.status.idle":"2022-03-31T08:27:39.113164Z","shell.execute_reply.started":"2022-03-31T08:27:39.103632Z","shell.execute_reply":"2022-03-31T08:27:39.112426Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class RetinaNetBoxLoss(tf.losses.Loss):\n    \"\"\"Implements Smooth L1 loss\"\"\"\n\n    def __init__(self, delta):\n        super(RetinaNetBoxLoss, self).__init__(\n            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n        )\n        self._delta = delta\n\n    def call(self, y_true, y_pred):\n        difference = y_true - y_pred\n        absolute_difference = tf.abs(difference)\n        squared_difference = difference ** 2\n        loss = tf.where(\n            tf.less(absolute_difference, self._delta),\n            0.5 * squared_difference,\n            absolute_difference - 0.5,\n        )\n        return tf.reduce_sum(loss, axis=-1)\n\n\nclass RetinaNetClassificationLoss(tf.losses.Loss):\n    \"\"\"Implements Focal loss\"\"\"\n\n    def __init__(self, alpha, gamma):\n        super(RetinaNetClassificationLoss, self).__init__(\n            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n        )\n        self._alpha = alpha\n        self._gamma = gamma\n\n    def call(self, y_true, y_pred):\n        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=y_true, logits=y_pred\n        )\n        probs = tf.nn.sigmoid(y_pred)\n        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n        return tf.reduce_sum(loss, axis=-1)\n\n\nclass RetinaNetLoss(tf.losses.Loss):\n    \"\"\"Wrapper to combine both the losses\"\"\"\n\n    def __init__(self, num_classes=10, alpha=0.25, gamma=2.0, delta=1.0):\n        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n        self._box_loss = RetinaNetBoxLoss(delta)\n        self._num_classes = num_classes\n\n    def call(self, y_true, y_pred):\n        y_pred = tf.cast(y_pred, dtype=tf.float32)\n        box_labels = y_true[:, :, :4]\n        box_predictions = y_pred[:, :, :4]\n        cls_labels = tf.one_hot(\n            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n            depth=self._num_classes,\n            dtype=tf.float32,\n        )\n        cls_predictions = y_pred[:, :, 4:]\n        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n        box_loss = self._box_loss(box_labels, box_predictions)\n        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n        loss = clf_loss + box_loss\n        return loss\n","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:40.853148Z","iopub.execute_input":"2022-03-31T08:27:40.853415Z","iopub.status.idle":"2022-03-31T08:27:40.871505Z","shell.execute_reply.started":"2022-03-31T08:27:40.853383Z","shell.execute_reply":"2022-03-31T08:27:40.870438Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"num_classes = 10\nbatch_size = 2\nlabel_encoder = LabelEncoder()\nmodel_dir='checpoint'\n\nbackbone = get_backbone()\nloss_fn = RetinaNetLoss(num_classes)\nmodel = Network(num_classes, backbone)\n\noptimizer = tf.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\nmodel.compile(loss=loss_fn, optimizer=optimizer)\n\ncallbacks_list = [\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n        monitor=\"loss\",\n        save_best_only=False,\n        save_weights_only=True,\n        verbose=1,\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:27:42.572433Z","iopub.execute_input":"2022-03-31T08:27:42.572934Z","iopub.status.idle":"2022-03-31T08:27:49.749502Z","shell.execute_reply.started":"2022-03-31T08:27:42.572893Z","shell.execute_reply":"2022-03-31T08:27:49.748699Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# parsing sample and extract features\ndef parse_tfrecord_fn(example):\n    feature_description = {\n            'image/height': tf.io.FixedLenFeature([], tf.int64),\n            'image/width': tf.io.FixedLenFeature([], tf.int64),\n            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n            'image/format': tf.io.FixedLenFeature([], tf.string),\n            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n            'image/object/class/label': tf.io.VarLenFeature(tf.int64)\n            }\n    example = tf.io.parse_single_example(example, feature_description)\n    example['image'] = tf.io.decode_jpeg(example['image/encoded'])\n    example['bbox']  = tf.stack([tf.sparse.to_dense(example['image/object/bbox/xmin']),\n                                tf.sparse.to_dense(example['image/object/bbox/ymin']),\n                                tf.sparse.to_dense(example['image/object/bbox/xmax']),\n                                tf.sparse.to_dense(example['image/object/bbox/ymax']),\n                              ], axis=-1)\n    example['label']=  tf.cast(tf.sparse.to_dense(example[\"image/object/class/label\"]), tf.int32)\n    return example","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:28:20.816835Z","iopub.execute_input":"2022-03-31T08:28:20.817253Z","iopub.status.idle":"2022-03-31T08:28:20.830934Z","shell.execute_reply.started":"2022-03-31T08:28:20.817200Z","shell.execute_reply":"2022-03-31T08:28:20.829783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_file='./train/trainval.tfrecord'\nvalid_file = './valid/trainval.tfrecord'\nautotune = tf.data.AUTOTUNE\n#trian dataset\ntrain_dataset = tf.data.TFRecordDataset(train_file)\ntrain_dataset = train_dataset.map(parse_tfrecord_fn, num_parallel_calls=autotune)\n\nfor features in train_dataset.take(1):\n    for key in features.keys():\n        if key != \"image/encoded\" and key != \"image\" and key != \"image/object/bbox/xmax\" and key != \"image/object/bbox/xmin\" and key != \"image/object/bbox/ymax\" and key != \"image/object/bbox/ymin\" and key != \"image/object/class/label\":\n            print(f\"{key}: {features[key]}\")\n\n    print(f\"Image shape: {features['image'].shape}\")\n    plt.figure(figsize=(7, 7))\n    plt.imshow(features[\"image\"].numpy())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T08:51:56.700483Z","iopub.execute_input":"2022-03-31T08:51:56.700880Z","iopub.status.idle":"2022-03-31T08:51:57.384968Z","shell.execute_reply.started":"2022-03-31T08:51:56.700730Z","shell.execute_reply":"2022-03-31T08:51:57.384309Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_file='./train/trainval.tfrecord'\nvalid_file = './valid/trainval.tfrecord'\nautotune = tf.data.AUTOTUNE\n\n#trian dataset\ntrain_dataset = tf.data.TFRecordDataset(train_file)\ntrain_dataset = train_dataset.map(parse_tfrecord_fn, num_parallel_calls=autotune)\ntrain_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\ntrain_dataset = train_dataset.shuffle(8 * batch_size)\ntrain_dataset = train_dataset.padded_batch(batch_size=batch_size)\ntrain_dataset = train_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\ntrain_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\ntrain_dataset = train_dataset.prefetch(autotune)\n\n# validation dataset\nval_dataset = tf.data.TFRecordDataset(valid_file)\nval_dataset = val_dataset.map(parse_tfrecord_fn, num_parallel_calls=autotune)\nval_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\nval_dataset = val_dataset.padded_batch(batch_size=1)\nval_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\nval_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\nval_dataset = val_dataset.prefetch(autotune)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T09:04:36.498120Z","iopub.execute_input":"2022-03-31T09:04:36.499005Z","iopub.status.idle":"2022-03-31T09:04:37.395524Z","shell.execute_reply.started":"2022-03-31T09:04:36.498958Z","shell.execute_reply":"2022-03-31T09:04:37.394824Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\nepochs = 1\n\n# Running 80 training and 20 validation steps samples .\n\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epochs,\n    callbacks=callbacks_list,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T09:04:45.938608Z","iopub.execute_input":"2022-03-31T09:04:45.938891Z","iopub.status.idle":"2022-03-31T09:05:24.460682Z","shell.execute_reply.started":"2022-03-31T09:04:45.938835Z","shell.execute_reply":"2022-03-31T09:05:24.459905Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}