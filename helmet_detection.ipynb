{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Helmet Detection\n","## By Saleh Shmali   \n","Email: salehshmali@outlook.com  \n","Phone: (+971) 503756709\n","\n","## Dfine the problem\n","for helmet detection  i downloaded videos for Myanmar street where is a lot of motocycle in it's streets then i annotated labels just like this:\n","\n","DHelmet, DHelmetP1Helmet, DHelmetP1NoHelmet, DN0HelmetP1NoHelmetP2NoHelmet, DNoHelmet, DNoHelmetP0NoHelmet, DNoHelmetP1Helmet, DNoHelmetP1NoHelmet, DNoHelmetP1NoHelmetP2NoHelmet, DNoHelmetPoNoHelmetP1NoHelmet\n","\n","Which D:Driver , P1:First Rider , P2:Second Rider , P0 :Kid Rider in front of the driver\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:50:12.413797Z","iopub.status.busy":"2022-03-31T08:50:12.413532Z","iopub.status.idle":"2022-03-31T08:50:12.417789Z","shell.execute_reply":"2022-03-31T08:50:12.417002Z","shell.execute_reply.started":"2022-03-31T08:50:12.413768Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# Data preprocessing\n","\n","Impliminting some functions for image and bbox prepearring"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:12.084726Z","iopub.status.busy":"2022-03-31T08:27:12.084454Z","iopub.status.idle":"2022-03-31T08:27:12.093472Z","shell.execute_reply":"2022-03-31T08:27:12.092819Z","shell.execute_reply.started":"2022-03-31T08:27:12.084696Z"},"trusted":true},"outputs":[],"source":["\n","def swap_xy(boxes):\n","    #Swaps order the of x and y coordinates of the boxes.\n","\n","    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n","\n","\n","def convert_to_xywh(boxes):\n","    #Changes the box format to center, width and height.\n","\n","    return tf.concat(\n","        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n","        axis=-1,\n","    )\n","\n","\n","def convert_to_corners(boxes):\n","    #Changes the box format to corner coordinates\n","\n","    return tf.concat(\n","        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n","        axis=-1,\n","    )\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:14.545288Z","iopub.status.busy":"2022-03-31T08:27:14.544438Z","iopub.status.idle":"2022-03-31T08:27:14.568415Z","shell.execute_reply":"2022-03-31T08:27:14.567060Z","shell.execute_reply.started":"2022-03-31T08:27:14.545220Z"},"trusted":true},"outputs":[],"source":["def compute_iou(boxes1, boxes2):\n","    #Computes pairwise IOU matrix for given two sets of boxes\n","\n","    boxes1_corners = convert_to_corners(boxes1)\n","    boxes2_corners = convert_to_corners(boxes2)\n","    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n","    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n","    intersection = tf.maximum(0.0, rd - lu)\n","    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n","    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n","    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n","    union_area = tf.maximum(\n","        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n","    )\n","    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n","\n","\n","def visualize_detections(\n","    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n","):\n","    \"\"\"Visualize Detections\"\"\"\n","    image = np.array(image, dtype=np.uint8)\n","    plt.figure(figsize=figsize)\n","    plt.axis(\"off\")\n","    plt.imshow(image)\n","    ax = plt.gca()\n","    for box, _cls, score in zip(boxes, classes, scores):\n","        text = \"{}: {:.2f}\".format(_cls, score)\n","        x1, y1, x2, y2 = box\n","        w, h = x2 - x1, y2 - y1\n","        patch = plt.Rectangle(\n","            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n","        )\n","        ax.add_patch(patch)\n","        ax.text(\n","            x1,\n","            y1,\n","            text,\n","            bbox={\"facecolor\": color, \"alpha\": 0.4},\n","            clip_box=ax.clipbox,\n","            clip_on=True,\n","        )\n","    plt.show()\n","    return ax"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:16.960531Z","iopub.status.busy":"2022-03-31T08:27:16.960270Z","iopub.status.idle":"2022-03-31T08:27:16.974451Z","shell.execute_reply":"2022-03-31T08:27:16.973778Z","shell.execute_reply.started":"2022-03-31T08:27:16.960500Z"},"trusted":true},"outputs":[],"source":["def random_flip_horizontal(image, boxes):\n","    #Flips image and boxes horizontally with 50% chance\n","\n","    if tf.random.uniform(()) > 0.5:\n","        image = tf.image.flip_left_right(image)\n","        boxes = tf.stack(\n","            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n","        )\n","    return image, boxes\n","\n","\n","def resize_and_pad_image(\n","    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n","):\n","    #Resizes and pads image while preserving aspect ratio.\n","\n","    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n","    if jitter is not None:\n","        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n","    ratio = min_side / tf.reduce_min(image_shape)\n","    if ratio * tf.reduce_max(image_shape) > max_side:\n","        ratio = max_side / tf.reduce_max(image_shape)\n","    image_shape = ratio * image_shape\n","    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n","    padded_image_shape = tf.cast(\n","        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n","    )\n","    image = tf.image.pad_to_bounding_box(\n","        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n","    )\n","    return image, image_shape, ratio\n","\n","\n","def preprocess_data(sample):\n","  \n","    image=sample['image']\n","    bbox=sample['bbox']\n","    class_id=sample['label']\n","    bbox = swap_xy(bbox)\n","    image, bbox = random_flip_horizontal(image, bbox)\n","    image, image_shape, _ = resize_and_pad_image(image)\n","\n","    bbox = tf.stack(\n","        [\n","            bbox[:, 0] * image_shape[1],\n","            bbox[:, 1] * image_shape[0],\n","            bbox[:, 2] * image_shape[1],\n","            bbox[:, 3] * image_shape[0],\n","        ],\n","        axis=-1,\n","    )\n","    bbox = convert_to_xywh(bbox)\n","    return image, bbox, class_id\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:30.213467Z","iopub.status.busy":"2022-03-31T08:27:30.213186Z","iopub.status.idle":"2022-03-31T08:27:31.225976Z","shell.execute_reply":"2022-03-31T08:27:31.225241Z","shell.execute_reply.started":"2022-03-31T08:27:30.213437Z"},"trusted":true},"outputs":[],"source":["class DecodePredictions(tf.keras.layers.Layer):\n","    #A Keras layer that decodes predictions of the RetinaNet model.\n","\n","    def __init__(\n","        self,\n","        num_classes=80,\n","        confidence_threshold=0.05,\n","        nms_iou_threshold=0.5,\n","        max_detections_per_class=100,\n","        max_detections=100,\n","        box_variance=[0.1, 0.1, 0.2, 0.2],\n","        **kwargs\n","    ):\n","        super(DecodePredictions, self).__init__(**kwargs)\n","        self.num_classes = num_classes\n","        self.confidence_threshold = confidence_threshold\n","        self.nms_iou_threshold = nms_iou_threshold\n","        self.max_detections_per_class = max_detections_per_class\n","        self.max_detections = max_detections\n","\n","        self._anchor_box = AnchorBox()\n","        self._box_variance = tf.convert_to_tensor(\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n","        )\n","\n","    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n","        boxes = box_predictions * self._box_variance\n","        boxes = tf.concat(\n","            [\n","                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n","                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n","            ],\n","            axis=-1,\n","        )\n","        boxes_transformed = convert_to_corners(boxes)\n","        return boxes_transformed\n","\n","    def call(self, images, predictions):\n","        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n","        box_predictions = predictions[:, :, :4]\n","        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n","        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n","\n","        return tf.image.combined_non_max_suppression(\n","            tf.expand_dims(boxes, axis=2),\n","            cls_predictions,\n","            self.max_detections_per_class,\n","            self.max_detections,\n","            self.nms_iou_threshold,\n","            self.confidence_threshold,\n","            clip_boxes=False,\n","        )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:31.519331Z","iopub.status.busy":"2022-03-31T08:27:31.519008Z","iopub.status.idle":"2022-03-31T08:27:31.535866Z","shell.execute_reply":"2022-03-31T08:27:31.535276Z","shell.execute_reply.started":"2022-03-31T08:27:31.519302Z"},"trusted":true},"outputs":[],"source":["class AnchorBox:\n","    # Generates anchor boxes.\n","\n","    def __init__(self):\n","        self.aspect_ratios = [0.5, 1.0, 2.0]\n","        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n","\n","        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n","        self._strides = [2 ** i for i in range(3, 8)]\n","        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n","        self._anchor_dims = self._compute_dims()\n","\n","    def _compute_dims(self):\n","        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n","        of the feature pyramid.\n","        \"\"\"\n","        anchor_dims_all = []\n","        for area in self._areas:\n","            anchor_dims = []\n","            for ratio in self.aspect_ratios:\n","                anchor_height = tf.math.sqrt(area / ratio)\n","                anchor_width = area / anchor_height\n","                dims = tf.reshape(\n","                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n","                )\n","                for scale in self.scales:\n","                    anchor_dims.append(scale * dims)\n","            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n","        return anchor_dims_all\n","\n","    def _get_anchors(self, feature_height, feature_width, level):\n","        #Generates anchor boxes for a given feature map size and level\n","\n","        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n","        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n","        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n","        centers = tf.expand_dims(centers, axis=-2)\n","        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n","        dims = tf.tile(\n","            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n","        )\n","        anchors = tf.concat([centers, dims], axis=-1)\n","        return tf.reshape(\n","            anchors, [feature_height * feature_width * self._num_anchors, 4]\n","        )\n","\n","    def get_anchors(self, image_height, image_width):\n","        # Generates anchor boxes for all the feature maps of the feature pyramid.\n","\n","        anchors = [\n","            self._get_anchors(\n","                tf.math.ceil(image_height / 2 ** i),\n","                tf.math.ceil(image_width / 2 ** i),\n","                i,\n","            )\n","            for i in range(3, 8)\n","        ]\n","        return tf.concat(anchors, axis=0)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:32.813310Z","iopub.status.busy":"2022-03-31T08:27:32.812882Z","iopub.status.idle":"2022-03-31T08:27:32.831366Z","shell.execute_reply":"2022-03-31T08:27:32.830473Z","shell.execute_reply.started":"2022-03-31T08:27:32.813269Z"},"trusted":true},"outputs":[],"source":["class LabelEncoder:\n","    #  Transforms the raw labels into targets for training.\n","\n","    def __init__(self):\n","        self._anchor_box = AnchorBox()\n","        self._box_variance = tf.convert_to_tensor(\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n","        )\n","\n","    def _match_anchor_boxes(\n","        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n","    ):\n","        #Matches ground truth boxes to anchor boxes based on IOU.\n","\n","        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n","        max_iou = tf.reduce_max(iou_matrix, axis=1)\n","        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n","        positive_mask = tf.greater_equal(max_iou, match_iou)\n","        negative_mask = tf.less(max_iou, ignore_iou)\n","        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n","        return (\n","            matched_gt_idx,\n","            tf.cast(positive_mask, dtype=tf.float32),\n","            tf.cast(ignore_mask, dtype=tf.float32),\n","        )\n","\n","    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n","        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n","        box_target = tf.concat(\n","            [\n","                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n","                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n","            ],\n","            axis=-1,\n","        )\n","        box_target = box_target / self._box_variance\n","        return box_target\n","\n","    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n","        \"\"\"Creates box and classification targets for a single sample\"\"\"\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n","        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n","        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n","            anchor_boxes, gt_boxes\n","        )\n","        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n","        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n","        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n","        cls_target = tf.where(\n","            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n","        )\n","        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n","        cls_target = tf.expand_dims(cls_target, axis=-1)\n","        label = tf.concat([box_target, cls_target], axis=-1)\n","        return label\n","\n","    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n","        \"\"\"Creates box and classification targets for a batch\"\"\"\n","        images_shape = tf.shape(batch_images)\n","        batch_size = images_shape[0]\n","\n","        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n","        for i in range(batch_size):\n","            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n","            labels = labels.write(i, label)\n","        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n","        return batch_images, labels.stack()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:34.148294Z","iopub.status.busy":"2022-03-31T08:27:34.147880Z","iopub.status.idle":"2022-03-31T08:27:34.154447Z","shell.execute_reply":"2022-03-31T08:27:34.153759Z","shell.execute_reply.started":"2022-03-31T08:27:34.148252Z"},"trusted":true},"outputs":[],"source":["def get_backbone():\n","    #Builds ResNet50 with pre-trained imagenet weights\n","    backbone = keras.applications.ResNet50(\n","        include_top=False, input_shape=[None, None, 3]\n","    )\n","    c3_output, c4_output, c5_output = [\n","        backbone.get_layer(layer_name).output\n","        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n","    ]\n","    return keras.Model(\n","        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n","    )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:35.851002Z","iopub.status.busy":"2022-03-31T08:27:35.850572Z","iopub.status.idle":"2022-03-31T08:27:35.862555Z","shell.execute_reply":"2022-03-31T08:27:35.861877Z","shell.execute_reply.started":"2022-03-31T08:27:35.850961Z"},"trusted":true},"outputs":[],"source":["\n","class FeaturePyramid(keras.layers.Layer):\n","    #Builds the Feature Pyramid with the feature maps from the backbone.\n","\n","    def __init__(self, backbone=None, **kwargs):\n","        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n","        self.backbone = backbone if backbone else get_backbone()\n","        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.upsample_2x = keras.layers.UpSampling2D(2)\n","\n","    def call(self, images, training=False):\n","        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n","        p3_output = self.conv_c3_1x1(c3_output)\n","        p4_output = self.conv_c4_1x1(c4_output)\n","        p5_output = self.conv_c5_1x1(c5_output)\n","        p4_output = p4_output + self.upsample_2x(p5_output)\n","        p3_output = p3_output + self.upsample_2x(p4_output)\n","        p3_output = self.conv_c3_3x3(p3_output)\n","        p4_output = self.conv_c4_3x3(p4_output)\n","        p5_output = self.conv_c5_3x3(p5_output)\n","        p6_output = self.conv_c6_3x3(c5_output)\n","        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n","        return p3_output, p4_output, p5_output, p6_output, p7_output\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:37.565961Z","iopub.status.busy":"2022-03-31T08:27:37.565217Z","iopub.status.idle":"2022-03-31T08:27:37.572456Z","shell.execute_reply":"2022-03-31T08:27:37.571765Z","shell.execute_reply.started":"2022-03-31T08:27:37.565926Z"},"trusted":true},"outputs":[],"source":["\n","def build_head(output_filters, bias_init):\n","    # Builds the class/box predictions head.\n","\n","    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n","    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n","    for _ in range(4):\n","        head.add(\n","            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n","        )\n","        head.add(keras.layers.ReLU())\n","    head.add(\n","        keras.layers.Conv2D(\n","            output_filters,\n","            3,\n","            1,\n","            padding=\"same\",\n","            kernel_initializer=kernel_init,\n","            bias_initializer=bias_init,\n","        )\n","    )\n","    return head"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:39.103672Z","iopub.status.busy":"2022-03-31T08:27:39.103144Z","iopub.status.idle":"2022-03-31T08:27:39.113164Z","shell.execute_reply":"2022-03-31T08:27:39.112426Z","shell.execute_reply.started":"2022-03-31T08:27:39.103632Z"},"trusted":true},"outputs":[],"source":["class Network(keras.Model):\n","    # A subclassed Keras model implementing the RetinaNet architecture.\n","\n","    def __init__(self, num_classes, backbone=None, **kwargs):\n","        super(Network, self).__init__(name=\"Network\", **kwargs)\n","        self.fpn = FeaturePyramid(backbone)\n","        self.num_classes = num_classes\n","\n","        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n","        self.cls_head = build_head(9 * num_classes, prior_probability)\n","        self.box_head = build_head(9 * 4, \"zeros\")\n","\n","    def call(self, image, training=False):\n","        features = self.fpn(image, training=training)\n","        N = tf.shape(image)[0]\n","        cls_outputs = []\n","        box_outputs = []\n","        for feature in features:\n","            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n","            cls_outputs.append(\n","                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n","            )\n","        cls_outputs = tf.concat(cls_outputs, axis=1)\n","        box_outputs = tf.concat(box_outputs, axis=1)\n","        return tf.concat([box_outputs, cls_outputs], axis=-1)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:40.853415Z","iopub.status.busy":"2022-03-31T08:27:40.853148Z","iopub.status.idle":"2022-03-31T08:27:40.871505Z","shell.execute_reply":"2022-03-31T08:27:40.870438Z","shell.execute_reply.started":"2022-03-31T08:27:40.853383Z"},"trusted":true},"outputs":[],"source":["class RetinaNetBoxLoss(tf.losses.Loss):\n","    \"\"\"Implements Smooth L1 loss\"\"\"\n","\n","    def __init__(self, delta):\n","        super(RetinaNetBoxLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n","        )\n","        self._delta = delta\n","\n","    def call(self, y_true, y_pred):\n","        difference = y_true - y_pred\n","        absolute_difference = tf.abs(difference)\n","        squared_difference = difference ** 2\n","        loss = tf.where(\n","            tf.less(absolute_difference, self._delta),\n","            0.5 * squared_difference,\n","            absolute_difference - 0.5,\n","        )\n","        return tf.reduce_sum(loss, axis=-1)\n","\n","\n","class RetinaNetClassificationLoss(tf.losses.Loss):\n","    \"\"\"Implements Focal loss\"\"\"\n","\n","    def __init__(self, alpha, gamma):\n","        super(RetinaNetClassificationLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n","        )\n","        self._alpha = alpha\n","        self._gamma = gamma\n","\n","    def call(self, y_true, y_pred):\n","        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=y_true, logits=y_pred\n","        )\n","        probs = tf.nn.sigmoid(y_pred)\n","        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n","        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n","        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n","        return tf.reduce_sum(loss, axis=-1)\n","\n","\n","class RetinaNetLoss(tf.losses.Loss):\n","    \"\"\"Wrapper to combine both the losses\"\"\"\n","\n","    def __init__(self, num_classes=10, alpha=0.25, gamma=2.0, delta=1.0):\n","        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n","        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n","        self._box_loss = RetinaNetBoxLoss(delta)\n","        self._num_classes = num_classes\n","\n","    def call(self, y_true, y_pred):\n","        y_pred = tf.cast(y_pred, dtype=tf.float32)\n","        box_labels = y_true[:, :, :4]\n","        box_predictions = y_pred[:, :, :4]\n","        cls_labels = tf.one_hot(\n","            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n","            depth=self._num_classes,\n","            dtype=tf.float32,\n","        )\n","        cls_predictions = y_pred[:, :, 4:]\n","        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n","        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n","        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n","        box_loss = self._box_loss(box_labels, box_predictions)\n","        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n","        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n","        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n","        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n","        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n","        loss = clf_loss + box_loss\n","        return loss\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:27:42.572934Z","iopub.status.busy":"2022-03-31T08:27:42.572433Z","iopub.status.idle":"2022-03-31T08:27:49.749502Z","shell.execute_reply":"2022-03-31T08:27:49.748699Z","shell.execute_reply.started":"2022-03-31T08:27:42.572893Z"},"trusted":true},"outputs":[],"source":["num_classes = 10\n","batch_size = 2\n","label_encoder = LabelEncoder()\n","model_dir='checpoint'\n","\n","backbone = get_backbone()\n","loss_fn = RetinaNetLoss(num_classes)\n","model = Network(num_classes, backbone)\n","\n","optimizer = tf.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n","model.compile(loss=loss_fn, optimizer=optimizer)\n","\n","callbacks_list = [\n","    tf.keras.callbacks.ModelCheckpoint(\n","        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n","        monitor=\"loss\",\n","        save_best_only=False,\n","        save_weights_only=True,\n","        verbose=1,\n","    )\n","]"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T08:28:20.817253Z","iopub.status.busy":"2022-03-31T08:28:20.816835Z","iopub.status.idle":"2022-03-31T08:28:20.830934Z","shell.execute_reply":"2022-03-31T08:28:20.829783Z","shell.execute_reply.started":"2022-03-31T08:28:20.817200Z"},"trusted":true},"outputs":[],"source":["# parsing sample and extract features\n","def parse_tfrecord_fn(example):\n","    feature_description = {\n","            'image/height': tf.io.FixedLenFeature([], tf.int64),\n","            'image/width': tf.io.FixedLenFeature([], tf.int64),\n","            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","            'image/format': tf.io.FixedLenFeature([], tf.string),\n","            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n","            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n","            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n","            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n","            'image/object/class/label': tf.io.VarLenFeature(tf.int64)\n","            }\n","    example = tf.io.parse_single_example(example, feature_description)\n","    example['image'] = tf.io.decode_jpeg(example['image/encoded'])\n","    example['bbox']  = tf.stack([tf.sparse.to_dense(example['image/object/bbox/xmin']),\n","                                tf.sparse.to_dense(example['image/object/bbox/ymin']),\n","                                tf.sparse.to_dense(example['image/object/bbox/xmax']),\n","                                tf.sparse.to_dense(example['image/object/bbox/ymax']),\n","                              ], axis=-1)\n","    example['label']=  tf.cast(tf.sparse.to_dense(example[\"image/object/class/label\"]), tf.int32)\n","    return example"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T09:04:36.499005Z","iopub.status.busy":"2022-03-31T09:04:36.498120Z","iopub.status.idle":"2022-03-31T09:04:37.395524Z","shell.execute_reply":"2022-03-31T09:04:37.394824Z","shell.execute_reply.started":"2022-03-31T09:04:36.498958Z"},"trusted":true},"outputs":[],"source":["train_file='./data/train/trainval.tfrecord'\n","valid_file = './data/valid/trainval.tfrecord'\n","autotune = tf.data.AUTOTUNE\n","\n","#trian dataset\n","train_dataset = tf.data.TFRecordDataset(train_file)\n","train_dataset = train_dataset.map(parse_tfrecord_fn, num_parallel_calls=autotune)\n","train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n","train_dataset = train_dataset.shuffle(8 * batch_size)\n","train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n","train_dataset = train_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n","train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n","train_dataset = train_dataset.prefetch(autotune)\n","\n","# validation dataset\n","val_dataset = tf.data.TFRecordDataset(valid_file)\n","val_dataset = val_dataset.map(parse_tfrecord_fn, num_parallel_calls=autotune)\n","val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n","val_dataset = val_dataset.padded_batch(batch_size=1)\n","val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n","val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n","val_dataset = val_dataset.prefetch(autotune)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-03-31T09:04:45.938891Z","iopub.status.busy":"2022-03-31T09:04:45.938608Z","iopub.status.idle":"2022-03-31T09:05:24.460682Z","shell.execute_reply":"2022-03-31T09:05:24.459905Z","shell.execute_reply.started":"2022-03-31T09:04:45.938835Z"},"trusted":true},"outputs":[],"source":["\n","epochs = 1\n","\n","# Running 80 training and 20 validation steps samples .\n","\n","model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=epochs,\n","    callbacks=callbacks_list,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
